\documentclass[usenames,dvipsnames]{beamer}

\usepackage{tikz}
\usepackage{latexsym,pdfsync,xcolor,graphicx}
\usepackage{ragged2e}

\usetheme{Madrid} 
\usecolortheme{seahorse} 


%código para la línea temporal
\setbeamercovered{invisible}
\usetikzlibrary{overlay-beamer-styles}
\tikzset{
    highlight on/.style={alt={#1{fill=red!80!black,color=red!80!black}{fill=gray!30!white,color=gray!30!white}}},
}

%--------------------------------------------------------



\title[Semantic Textual Similarity]{Semantic Textual Similarity}
\author{Mauro Vázquez Chas, Dániel Mácsai}
\date{December 12, 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\section{Preprocessing}
\begin{frame}{Preprocessing}
    \begin{block}{Preprocessing}
        \begin{itemize}
            \item Lowercasing and removing punctuation
            \item Tokenization with and without stopwords
            \item Lemmatization with and without stopwords
            \item Synsets computed by Lesk's algorithm
        \end{itemize}
    \end{block}
\end{frame}

\section{Features computed}

\begin{frame}{Basic features}
    \begin{block}{Basic features}
        \begin{itemize}
            \item Character ratio, token ratio
            \item Levenshtein ratio (edit distance)
            \item Jaccard similarity
                \begin{itemize}
                    \item on tokens with and without stopwords 
                    \item on lemmas with and without stopwords
                    \item on synsets from Lesk's algorithm
                \end{itemize}   
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Utilizing synset similarity}
    \begin{block}{Best possible pair}
        For each word in each sentence, we get the best similarity value considering all of its possible synsets and all the synsets from the words in the other sentence, all of this without modifying its post tags. After this, take the average, considering only the tokens with a valid wordnet pos tag.
        Methods we used for the synset similarities: 
        \begin{itemize}
            \item Path similarity
            \item Wu-Palmer similarity
            \item Leacock-Chodorow similarity
        \end{itemize}
    \end{block}
     
\end{frame}
\begin{frame}{Utilizing synset similarity}
    \begin{block}{Using Lesk's algorithm}
        We use Lesk's algorithm to match synsets to tokens in the two sentences.
        After, for each synset in a sentence, we compute the highest similarity value using the synsets from the other sentence, and we take the average of these similarities.
        Methods we used for the synset similarities: 
        \begin{itemize}
            \item Path similarity
            \item Wu-Palmer similarity
            \item Leacock-Chodorow similarity
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Other features}
    \begin{block}{N-grams}
        \begin{itemize}
            \item On tokens, lemmas and characters
            \item Using Jaccard similarity for sets and cosine similarities on histograms
        \end{itemize}
    \end{block}
    \begin{block}{SentiWordNet}
        For both the positive and the negative scores we compute the following getting two features in total:
        For each sentence, we sum the scores of each Lesk synset. Afterwards, we substract the values of the sentences and normalize by the maximum number of synsets between the two sentences.
    \end{block}
\end{frame}

\section{Models and additional ideas}
\begin{frame}{Models and additional ideas}
    \begin{block}{Models}
        \begin{itemize}
            \item SVM
            \item XGBoost
            \item Random Forest
        \end{itemize}
    \end{block}

    \begin{block}{Additional ideas}
        \begin{itemize}
            \item Oversampling SMTeuroparl and MSRpar
            \item Feature selection with Random Forest based on importance
            \item Normalizing the features
            \item PCA on the features
        \end{itemize}
    \end{block}
\end{frame}

\section{Course of Actions}

\begin{frame}{Course of Actions}
    \begin{block}{Initial approach}
        \begin{itemize}
            \item Features
                \begin{itemize}
                    \item Basic features (without Levenshtein)
                    \item Synset best possible pair similarity (Path and Wu-Palmer)
                    \item Jaccard similarities for tokens and lemmas
                    \item N-grams jaccard similarities
                \end{itemize}
            \item Using XGBoost (with a grid search for hyperparameters)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Course of Actions}
    \frametitle{Course of Actions}
    \begin{columns}
    \column{0.5\textwidth}
        \begin{itemize}
            \item \textcolor{ForestGreen}{Switched to Random Forest (with a grid search for hyperparameters)}
            \pause
            \item \textcolor{red}{Manual feature selection}
            \pause
            \item \textcolor{ForestGreen}{Added Leacock-Chodorow similarity}
            \pause
            \item \textcolor{red}{Added Resnik and Jiang-Conrath similarities}
            \pause
            \item \textcolor{red}{Tried Support Vector Regression}
            \pause
            \item \textcolor{red}{Tried Oversampling}
            \pause
            \item \textcolor{ForestGreen}{Added PCA}
            \item \textcolor{ForestGreen}{Lesk Jaccard similarities}
            \pause
        \end{itemize}   
    \column{0.5\textwidth}
        \begin{itemize}
            \item \textcolor{ForestGreen}{SentiWordNet}
            \pause
            \item \textcolor{ForestGreen}{Added N-Grams occurrence-based similarities}
            \pause
            \item \textcolor{ForestGreen}{Ignore stopwords in Jaccard similarities}
            \pause
            \item \textcolor{red}{Tried feature selection with Random Forest Importance}
            \pause
            \item \textcolor{ForestGreen}{Removed punctuation}
            \pause
            \item \textcolor{ForestGreen}{Added lesk synset pair similarities}
            \pause
            \item \textcolor{red}{Tried Scaling Variables}
            \pause
            \item \textcolor{ForestGreen}{Removed PCA}
        \end{itemize}
    \end{columns}
\end{frame}

\section{Results}

\begin{frame}{Results}
    \begin{table}[h]
        \centering
        \begin{tabular}{|l|c|}
            \hline
            \textbf{Dataset} & \textbf{Pearson Correlation} \\ \hline
            SMTeuroparl & 0.5794 \\ \hline
            MSRvid & 0.8375 \\ \hline
            MSRpar & 0.6148 \\ \hline
            surprise.OnWN & 0.7240 \\ \hline
            surprise.SMTnews & 0.5576 \\ \hline
            All datasets & 0.7382 \\ \hline
        \end{tabular}
        \caption{Testing Pearson Correlation on Different Datasets}
        \label{tab:pearson-correlation}
    \end{table}  
\end{frame}


% Section 4: Discussion and Conclusion
\section{Takeaways}

\begin{frame}{Takeaways}
    \begin{itemize}
        \item Impact of the different Features
        \item Gap with modern techniques
        \item Importance of cleaning and preprocessing
        \item Importance of effective implementation
        \item Importance of the choice of the model
    \end{itemize}
\end{frame}

% Section 5: Questions
\begin{frame}{Questions}
    \centering
    {\Huge \textbf{Questions?}}
\end{frame}

\end{document}
