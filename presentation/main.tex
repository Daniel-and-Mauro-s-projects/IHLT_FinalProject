\documentclass{beamer}

\usetheme{Madrid} 
\usecolortheme{seahorse} 

\title[Semantic Textual Similarity]{Semantic Textual Similarity}
\author{Mauro Vázquez Chas, Dániel Mácsai}
\date{December 12, 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\section{Preprocessing}
\begin{frame}{Preprocessing}
    \begin{block}{Precomputation}
        \begin{itemize}
            \item Lowercasing and removing punctuation
            \item Tokenization with and without stopwords
            \item Lemmatization with and without stopwords
            \item Synsets computed by Lesk's algorithm
        \end{itemize}
    \end{block}
\end{frame}

\section{Features computed}

\begin{frame}{Basic features}
    \begin{block}{Basic features}
        \begin{itemize}
            \item Character ratio, token ratio
            \item Levenshtein ratio (edit distance)
            \item Jaccard similarity
                \begin{itemize}
                    \item on tokens with and without stopwords 
                    \item on lemmas with and without stopwords
                    \item on synsets from Lesk's algorithm
                \end{itemize}   
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Utilizing synset similarity}
    \begin{block}{Best possible pair}
        For each word in each sentence, we get the best similarity value considering all of its possible synsets and all of the synsets from the words in the other sentence, all of this without modifying its post tags. After this, take the average, considering only the tokens with a valid wordnet pos tag.
        Methods we used for the synset similarities: 
        \begin{itemize}
            \item Path similarity
            \item Wu-Palmer similarity
            \item Leacock-Chodorow similarity
        \end{itemize}
    \end{block}
     
\end{frame}
\begin{frame}{Utilizing synset similarity}
    \begin{block}{Using Lesk's algorithm}
        We use Lesk's algorithm to match synsets to tokens in the two sentences.
        After, for each synset in a sentence, we compute the highest similarity value using the synsets from the other sentence, and we take the average of these similarities.
        Methods we used for the synset similarities: 
        \begin{itemize}
            \item Path similarity
            \item Wu-Palmer similarity
            \item Leacock-Chodorow similarity
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Other features}
    \begin{block}{N-grams}
        \begin{itemize}
            \item On tokens, lemmas and characters
            \item Using Jaccard similarity for sets and cosine similarities on histograms
        \end{itemize}
    \end{block}
    \begin{block}{SentiWordNet}
        For both the positive and the negative scores we compute the following getting two features in total:
        For each sentence, we sum the scores of each Lesk synset. Afterwards, we substract the values of the sentences and normalize by the maximum number of synsets between the two sentences.
    \end{block}
\end{frame}

\section{Models and additional ideas}
\begin{frame}{Models and additional ideas}
    \begin{block}{Models}
        \begin{itemize}
            \item SVM
            \item XGBoost
            \item Random Forest
        \end{itemize}
    \end{block}

    \begin{block}{Additional ideas}
        \begin{itemize}
            \item Oversampling SMTeuroparl and MSRpar
            \item Feature selection with Random Forest based on importance
            \item Normalizing the features
            \item PCA on the features
        \end{itemize}
    \end{block}
\end{frame}

\section{Results}

\begin{frame}{Results}
    \begin{table}[h]
        \centering
        \begin{tabular}{|l|c|}
            \hline
            \textbf{Dataset} & \textbf{Pearson Correlation} \\ \hline
            SMTeuroparl & 0.5794 \\ \hline
            MSRvid & 0.8375 \\ \hline
            MSRpar & 0.6148 \\ \hline
            surprise.OnWN & 0.7240 \\ \hline
            surprise.SMTnews & 0.5576 \\ \hline
            All datasets & 0.7382 \\ \hline
        \end{tabular}
        \caption{Testing Pearson Correlation on Different Datasets}
        \label{tab:pearson-correlation}
    \end{table}  
\end{frame}

% Section 4: Discussion and Conclusion
\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
    \end{itemize}
\end{frame}

% Section 5: Questions
\begin{frame}{Questions}
    \centering
    {\Huge \textbf{Questions?}}
\end{frame}

\end{document}
