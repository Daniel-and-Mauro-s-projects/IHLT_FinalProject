{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mauro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /home/mauro/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/mauro/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk import ngrams\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('sentiwordnet')\n",
    "ic_brown = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_of_elements(tokenized_sentence1, tokenized_sentence2) -> float:\n",
    "    \"\"\"Given two lists, return the ratio of the number of elements between the two lists\"\"\"\n",
    "    l1 = len(tokenized_sentence1)\n",
    "    l2 = len(tokenized_sentence2)\n",
    "    return min(l1,l2)/max(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levenstein_ratio(sentence1: str, sentence2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using the Levenshtein ratio.\n",
    "\n",
    "    The similarity is calculated as the ratio of the Levenshtein distance between the two sentences and the sum of the lengths of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Levenshtein ratio between the two sentences.\n",
    "    \"\"\"\n",
    "    return 1-(Levenshtein.distance(sentence1, sentence2)/max(len(sentence1), len(sentence2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_similarity(sentence1,sentence2) -> float:\n",
    "    \"\"\"Returns Jaccard similarity between lists (tokens, lemmas, synsets,...)\"\"\"\n",
    "    return 1-jaccard_distance(set(sentence1), set(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_methods = {\n",
    "    \"wu-palmer\": lambda s1, s2: s1.wup_similarity(s2),\n",
    "    \"path\": lambda s1, s2: s1.path_similarity(s2),\n",
    "    \"leacock\": lambda s1, s2: s1.lch_similarity(s2),\n",
    "    \"lin\": lambda s1, s2: s1.lin_similarity(s2, ic_brown)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(category):\n",
    "    \"\"\"\n",
    "    Convert a POS tag from the Spacy tagset to the WordNet tagset.\n",
    "    \"\"\"\n",
    "    if category.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif category.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif category.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif (category.startswith('R')) and (category != 'RP'):\n",
    "        # I looked into the RP tag is for particles\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return None  # WordNet doesn't handle other POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def get_best_synset_pair(word1, word2, pos, similarity_type) -> float:\n",
    "    \"\"\"\n",
    "    Get the best synset pair for two words.\n",
    "\n",
    "    This function gets the best synset pair for two words, considering every possible pair of synsets from the two words. The best pair is the one with the highest similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - word1: The first word, represented as a Spacy token.\n",
    "    - word2: The second word, represented as a Spacy token.\n",
    "    - pos: A string indicating the part of speech of the words.\n",
    "    - similarity_type: A string indicating the name of the similarity measure.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the similarity score of the best synset pair.\n",
    "    \"\"\"\n",
    "    # Create a cache key (I do this because it is simetric and to save time)\n",
    "    cache_key = tuple(sorted([word1.text, word2.text]) + [similarity_type]) \n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "\n",
    "    synsets_word1 = wordnet.synsets(word1.text, pos=pos)\n",
    "    synsets_word2 = wordnet.synsets(word2.text, pos=pos)\n",
    "\n",
    "    max_sim = 0\n",
    "    for synset1 in synsets_word1:\n",
    "        for synset2 in synsets_word2:\n",
    "            # We skip the satellite adjectives (gives problems for Lin and Leacock)\n",
    "            if synset1.pos() == 's' or synset2.pos() == 's':\n",
    "                continue\n",
    "            sim = similarity_methods[similarity_type](synset1, synset2)\n",
    "            if sim and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    \n",
    "    cache[cache_key] = max_sim\n",
    "    # If there is no similarity, we return 0\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_similarities(sentence1, sentence2, similarity_type) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using a specified similarity measure.\n",
    "\n",
    "    For each sentence, the function gets the best similarity value for each word considering every posible pair, using words from the other sentence. Then the mean of this similarities is computed over the sentence (normalizing by the number of tokens with a valid wordnet postag, not the total number of words) and the output is the mean similarity of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens.\n",
    "    - sentence2: The second sentence, represented as a list of tokens.\n",
    "    - similarity_type: A string indicating the type of similarity measure to use. \n",
    "        Options include \"wu-palmer\", \"path\", \"leacock\", and \"lin\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the average similarity score between the two sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do it for sentence1\n",
    "    similarity1 = 0\n",
    "    den = 0 # We will normalize by the number of words that have a valid POS for WordNet\n",
    "    for token1 in sentence1:\n",
    "        similarities = np.array([])\n",
    "        pos1 = get_wordnet_pos(token1.tag_)\n",
    "        if not pos1:\n",
    "            continue\n",
    "        den +=1\n",
    "        for token2 in sentence2:\n",
    "            pos2 = get_wordnet_pos(token2.tag_)\n",
    "            if (not pos2) or (pos1 != pos2):\n",
    "                continue\n",
    "            similarities = np.append(similarities,get_best_synset_pair(token1, token2, pos1, similarity_type))\n",
    "        if similarities.size > 0:\n",
    "            similarity1 += np.max(similarities)\n",
    "    # We average the similarity (even if they don't get a similarity)\n",
    "    similarity1 = similarity1 / den\n",
    "\n",
    "    # Do it for sentence2\n",
    "    similarity2 = 0\n",
    "    den = 0 # We will normalize by the number of words that have a valid POS for WordNet\n",
    "    for token2 in sentence2:\n",
    "        similarities = np.array([])\n",
    "        pos2 = get_wordnet_pos(token2.tag_)\n",
    "        if not pos2:\n",
    "            continue\n",
    "        den +=1\n",
    "        for token1 in sentence1:\n",
    "            pos1 = get_wordnet_pos(token1.tag_)\n",
    "            if (not pos1) or (pos1 != pos2):\n",
    "                continue\n",
    "            similarities = np.append(similarities, get_best_synset_pair(token1, token2, pos2, similarity_type))\n",
    "        if similarities.size > 0:\n",
    "            similarity2 += np.max(similarities)\n",
    "    # We average the similarity (even if they don't get a similarity)\n",
    "    similarity2 = similarity2 / den\n",
    "    \n",
    "    return np.mean(np.array([similarity1, similarity2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_sentence_similarity(synsets1, synsets2, similarity_type: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two lists of synsets using a specified similarity measure.\n",
    "\n",
    "    For each synset in the first list, the function gets the maximum similarity value with any synset in the second list. Then, the same is done for the second list. The output is the mean of this similarities over all synsets in the two lists.\n",
    "\n",
    "    Parameters:\n",
    "    - synsets1: The first list of synsets.\n",
    "    - synsets2: The second list of synsets.\n",
    "    - similarity_type: A string indicating the type of similarity measure to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the average similarity score between the two lists of synsets.\n",
    "    \"\"\"\n",
    "    assert similarity_type in similarity_methods.keys(), \"Invalid similarity type\"\n",
    "    similarities = np.array([])\n",
    "\n",
    "    for synset1 in synsets1:\n",
    "        if synset1: \n",
    "            max_sim = 0\n",
    "            for synset2 in synsets2:\n",
    "                if synset2:\n",
    "                    if synset1.pos() == synset2.pos():\n",
    "                        max_sim = max(max_sim, similarity_methods[similarity_type](synset1, synset2))\n",
    "            similarities = np.append(similarities, max_sim)\n",
    "    for synset2 in synsets2:\n",
    "        if synset2: \n",
    "            max_sim = 0\n",
    "            for synset1 in synsets1:\n",
    "                if synset1:\n",
    "                    if synset1.pos() == synset2.pos():\n",
    "                        max_sim = max(max_sim, similarity_methods[similarity_type](synset1, synset2))\n",
    "            similarities = np.append(similarities, max_sim)\n",
    "    if len(similarities) == 0:\n",
    "        return 0\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiwordnet_difference(synsets1,synsets2,method='pos') -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the absolute difference between the average sentiment scores of the lesk synsets of the two sentences, normalized by the maximum value of the sentiment score.\n",
    "\n",
    "    Parameters:\n",
    "    - synsets1: A list of synsets obtained by the lesk algorithm for the first sentence.\n",
    "    - synsets2: A list of synsets obtained by the lesk algorithm for the second sentence.\n",
    "    - method: A string indicating the type of sentiment score to use. \n",
    "        Options include \"pos\", \"neg\", and \"obj\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the difference in the mean sentiment scores of the two sentences, normalized by the maximum value of the sentiment score.\n",
    "    \"\"\"\n",
    "\n",
    "    sentisynsets1 = [swn.senti_synset(synset.name()) for synset in synsets1 if synset is not None]\n",
    "    sentisynsets2 = [swn.senti_synset(synset.name()) for synset in synsets2 if synset is not None]\n",
    "\n",
    "    l1 = len(sentisynsets1)\n",
    "    l2 = len(sentisynsets2)\n",
    "\n",
    "    if l1 == 0 or l2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    if method == 'pos':\n",
    "        sum1 = sum([sentiSynset.pos_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.pos_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    elif method == 'neg':\n",
    "        sum1 = sum([sentiSynset.neg_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.neg_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    elif method == 'obj':\n",
    "        sum1 = sum([sentiSynset.obj_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.obj_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    else:\n",
    "        raise ValueError(\"Error: this method is not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word N-Grams Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_jaccard_similarity(sentence1, sentence2, n) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using n-grams. The similarity is calculated as the jaccard similarity between the set of n-grams of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens or lemmas.\n",
    "    - sentence2: The second sentence, represented as a list of tokens or lemmas.\n",
    "    - n: The size of the n-grams to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Jaccard similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    ngrams1 = set([str(x) for x in list(ngrams(sentence1,n))])\n",
    "    ngrams2 = set([str(x) for x in list(ngrams(sentence2,n))])\n",
    "    if len(ngrams1) == 0 and len(ngrams2) == 0:\n",
    "        return 1\n",
    "    return 1-jaccard_distance(ngrams1,ngrams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_vector_similarity(sentence1, sentence2, n):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using n-grams.\n",
    "\n",
    "    For each sentence, we count the number of times each n-gram appears in the sentence. Then we sum the minimum of the counts of each n-gram match, multiply it by 2 and divide it by the sum of the counts of all n-grams in the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens/lemmas\n",
    "    - sentence2: The second sentence, represented as a list of tokens/lemmas\n",
    "    - n: The size of the n-grams to use.\n",
    "    \n",
    "    Returns:\n",
    "    - A float representing the similarity of the two sentences using n-grams.\n",
    "    \"\"\"\n",
    "    if len(sentence1) < n or len(sentence2) < n:\n",
    "        return 0\n",
    "    ngrams1 = [str(x) for x in list(ngrams(sentence1,n))]\n",
    "    ngrams2 = [str(x) for x in list(ngrams(sentence2,n))]\n",
    "    all_ngrams = set(ngrams1 + ngrams2) \n",
    "\n",
    "    c1 = Counter(ngrams1)\n",
    "    c2 = Counter(ngrams2)\n",
    "\n",
    "    coincidences = 0\n",
    "    for ngram in all_ngrams:\n",
    "        if ngram not in ngrams1 or ngram not in ngrams2:\n",
    "            continue\n",
    "        coincidences += min(c1[ngram],c2[ngram])\n",
    "    return 2*coincidences/(sum(c1.values())+sum(c2.values()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character N-Gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_ngram_jaccard_similarity(sentence1, sentence2, n):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using character n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using character n-grams. The similarity is calculated as the jaccard similarity between the set of n-grams of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "    - n: The size of the n-grams to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Jaccard similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    ngrams1 = set([str(x) for x in list(ngrams(sentence1,n))])\n",
    "    ngrams2 = set([str(x) for x in list(ngrams(sentence2,n))])\n",
    "\n",
    "    return 1-jaccard_distance(ngrams1,ngrams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_ngram_vector_cosine_similarity(sentence1: str, sentence2: str, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two sentences using character n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using character n-grams. \n",
    "    The similarity is calculated as the cosine similarity between the vectors of ocurrences of the n-grams in the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "    - nrange: the size of the ngrams.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the cosine similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    n_range = (n, n)\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=n_range)\n",
    "    ngrams = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity(ngrams[0], ngrams[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features (df):\n",
    "\n",
    "    features = pd.DataFrame()\n",
    "    for rowindex, row in df.iterrows():\n",
    "        # Basic features\n",
    "        features.at[rowindex, 'token_ratio'] = ratio_of_elements(row['0_nlp'], row['1_nlp'])\n",
    "        features.at[rowindex, 'char_ratio'] = ratio_of_elements(row[\"0_lower\"], row[\"1_lower\"])\n",
    "        features.at[rowindex, \"levenstein_ratio\"] = get_levenstein_ratio(row[\"0_lower\"], row[\"1_lower\"])\n",
    "\n",
    "        # Jaccard similarities\n",
    "        features.at[rowindex, 'jaccard_similarity_tokens'] = get_jaccard_similarity([token.text for token in row[\"0_nlp\"]], [token.text for token in row[\"1_nlp\"]])\n",
    "        features.at[rowindex, 'jaccard_similarity_tokens_no_stops'] = get_jaccard_similarity([token.text for token in row[\"0_nlp_no_stop\"]], [token.text for token in row[\"1_nlp_no_stop\"]])\n",
    "        features.at[rowindex, 'jaccard_similarity_lemmas'] = get_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"])\n",
    "        features.at[rowindex, 'jaccard_similarity_lemmas_no_stops'] = get_jaccard_similarity(row[\"0_lemma_no_stop\"], row[\"1_lemma_no_stop\"])\n",
    "        features.at[rowindex, 'jaccard_similarity_lesk'] = get_jaccard_similarity(row[\"0_lesk\"], row[\"1_lesk\"])\n",
    "\n",
    "        # Wordnet similarities (best synset pairs)\n",
    "        features.at[rowindex, 'wu-palmer_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"wu-palmer\")\n",
    "        #df.at[rowindex, 'lin_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"lin\")\n",
    "        features.at[rowindex, 'path_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"path\")\n",
    "        features.at[rowindex, 'leacock_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"leacock\")\n",
    "        \n",
    "        # Ngram vector similarities\n",
    "        features.at[rowindex, 'unigram_vector_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 1)\n",
    "        features.at[rowindex, 'bigram_vector_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'trigram_vector_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'quadgram_vector_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        # Ngram jaccard similarities tokens\n",
    "        features.at[rowindex, 'token_bigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 2)\n",
    "        features.at[rowindex, 'token_trigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 3)\n",
    "        features.at[rowindex, 'token_quadgram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 4)\n",
    "\n",
    "        # Ngram jaccard similarities lemmas\n",
    "        features.at[rowindex, 'lemma_bigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 2)\n",
    "        features.at[rowindex, 'lemma_trigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 3)\n",
    "        features.at[rowindex, 'lemma_quadgram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 4)\n",
    "\n",
    "        # Ngram characters jaccard similarities\n",
    "        features.at[rowindex, 'char_bigram_jaccard_similarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'char_trigram_jaccard_similarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'char_quadgram_jaccard_simmilarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        # Ngram character vector cosine similarities\n",
    "        features.at[rowindex, 'char_bigram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'char_trigram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'char_quadgram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        # Lesk wordnet similarities\n",
    "        features.at[rowindex, \"lesk_sentence_wu_palmer\"] = lesk_sentence_similarity(row[\"0_lesk\"], row[\"1_lesk\"], \"wu-palmer\")\n",
    "        features.at[rowindex, \"lesk_sentence_path\"] = lesk_sentence_similarity(row[\"0_lesk\"], row[\"1_lesk\"], \"path\")\n",
    "        features.at[rowindex, \"lesk_sentence_leacock\"] = lesk_sentence_similarity(row[\"0_lesk\"], row[\"1_lesk\"], \"leacock\")\n",
    "\n",
    "        # Sentiwordnet differences\n",
    "        features.at[rowindex, 'sentiwordnet_pos_difference'] = sentiwordnet_difference(row[\"0_lesk\"], row[\"1_lesk\"], 'pos')\n",
    "        features.at[rowindex, 'sentiwordnet_neg_difference'] = sentiwordnet_difference(row[\"0_lesk\"], row[\"1_lesk\"], 'neg')\n",
    "        # features.at[rowindex, 'sentiwordnet_obj_difference'] = sentiwordnet_correlation(row[\"0_nlp\"], row[\"1_nlp\"], 'obj') # It is directly dependent on the other two\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    \"\"\"Processes the sentences to get the features\"\"\"\n",
    "    # Lowercase\n",
    "    sentences[\"0_lower\"] = sentences[0].apply(lambda x: ''.join([char.lower() if not char.isdigit() else char for char in x]))\n",
    "    sentences[\"1_lower\"] = sentences[1].apply(lambda x: ''.join([char.lower() if not char.isdigit() else char for char in x]))\n",
    "\n",
    "    # Tokens\n",
    "    sentences[\"0_nlp\"] = sentences[\"0_lower\"].apply(lambda x: nlp(x))\n",
    "    sentences[\"1_nlp\"] = sentences[\"1_lower\"].apply(lambda x: nlp(x))\n",
    "    # Remove punctuation\n",
    "    sentences[\"0_nlp\"] = sentences[\"0_nlp\"].apply(lambda x: [token for token in x if not token.is_punct])\n",
    "    sentences[\"1_nlp\"] = sentences[\"1_nlp\"].apply(lambda x: [token for token in x if not token.is_punct])\n",
    "    \n",
    "    # Filter out stop words\n",
    "    sentences[\"0_nlp_no_stop\"] = sentences[\"0_nlp\"].apply(lambda x: [token for token in x if not token.is_stop])\n",
    "    sentences[\"1_nlp_no_stop\"] = sentences[\"1_nlp\"].apply(lambda x: [token for token in x if not token.is_stop])\n",
    "\n",
    "    # Lemmas\n",
    "    sentences['0_lemma'] = sentences['0_nlp'].apply(lambda x: [token.lemma for token in x])\n",
    "    sentences['1_lemma'] = sentences['1_nlp'].apply(lambda x: [token.lemma for token in x])\n",
    "    # Filter out stop words\n",
    "    sentences['0_lemma_no_stop'] = sentences['0_nlp_no_stop'].apply(lambda x: [token.lemma for token in x])\n",
    "    sentences['1_lemma_no_stop'] = sentences['1_nlp_no_stop'].apply(lambda x: [token.lemma for token in x])\n",
    "\n",
    "    # Lesk synsets\n",
    "    sentences[\"0_lesk\"] = sentences[\"0_nlp\"].apply(lambda x: [lesk(x, token.text, get_wordnet_pos(token.tag_)) for token in x])\n",
    "    sentences[\"1_lesk\"] = sentences[\"1_nlp\"].apply(lambda x: [lesk(x, token.text, get_wordnet_pos(token.tag_)) for token in x])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and clean data, tracking bad line indices\n",
    "def read_and_clean_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    bad_line_indices = []\n",
    "    for i, line in enumerate(lines):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields) == 2:\n",
    "            cleaned_lines.append(fields)\n",
    "        else:\n",
    "            print(f\"Skipping bad line: {line.strip()}\")\n",
    "            bad_line_indices.append(i)\n",
    "\n",
    "    return pd.DataFrame(cleaned_lines, columns=[0, 1]), bad_line_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "sentences_file_paths = [\n",
    "    \"../Data/Train/train/STS.input.SMTeuroparl.txt\",\n",
    "    \"../Data/Train/train/STS.input.MSRvid.txt\",\n",
    "    \"../Data/Train/train/STS.input.MSRpar.txt\",\n",
    "    ]\n",
    "gold_standard_file_paths = [\n",
    "    \"../Data/Train/train/STS.gs.SMTeuroparl.txt\",\n",
    "    \"../Data/Train/train/STS.gs.MSRvid.txt\",\n",
    "    \"../Data/Train/train/STS.gs.MSRpar.txt\",\n",
    "    ]   \n",
    "\n",
    "# Read and clean the data files into DataFrames, tracking bad line indices\n",
    "sentences_list = []\n",
    "bad_line_indices_list = []\n",
    "for path in sentences_file_paths:\n",
    "    df, bad_line_indices = read_and_clean_data(path)\n",
    "    sentences_list.append(df)\n",
    "    bad_line_indices_list.append(bad_line_indices)\n",
    "\n",
    "sentences_training: pd.DataFrame = pd.concat(sentences_list, ignore_index=True)\n",
    "\n",
    "gs_list = [pd.read_csv(path, header=None) for path in gold_standard_file_paths]\n",
    "# Remove the rows that correspond to bad lines\n",
    "for i, gs in enumerate(gs_list):\n",
    "    gs = gs.drop(bad_line_indices_list[i])\n",
    "gs_training: pd.DataFrame = pd.concat(gs_list, ignore_index=True)\n",
    "\n",
    "# Preprocess the sentences\n",
    "sentences_training = preprocessing(sentences_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features (~4mins)\n",
    "features_training = get_features(sentences_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sentences_file_paths_testing = [\n",
    "    \"../Data/Test/test-gold/STS.input.SMTeuroparl.txt\",\n",
    "    \"../Data/Test/test-gold/STS.input.MSRvid.txt\",\n",
    "    \"../Data/Test/test-gold/STS.input.MSRpar.txt\",\n",
    "    \"../Data/Test/test-gold/STS.input.surprise.OnWN.txt\",\n",
    "    \"../Data/Test/test-gold/STS.input.surprise.SMTnews.txt\",\n",
    "    ]\n",
    "gold_standard_file_paths_testing = [\n",
    "    \"../Data/Test/test-gold/STS.gs.SMTeuroparl.txt\",\n",
    "    \"../Data/Test/test-gold/STS.gs.MSRvid.txt\",\n",
    "    \"../Data/Test/test-gold/STS.gs.MSRpar.txt\",\n",
    "    \"../Data/Test/test-gold/STS.gs.surprise.OnWN.txt\",\n",
    "    \"../Data/Test/test-gold/STS.gs.surprise.SMTnews.txt\",\n",
    "    ]   \n",
    "\n",
    "# Read and clean the data files into DataFrames, tracking bad line indices\n",
    "sentences_list_testing = []\n",
    "bad_line_indices_list = []\n",
    "for path in sentences_file_paths_testing:\n",
    "    df, bad_line_indices = read_and_clean_data(path)\n",
    "    sentences_list_testing.append(df)\n",
    "    bad_line_indices_list.append(bad_line_indices)\n",
    "\n",
    "gs_list_testing = [pd.read_csv(path, header=None) for path in gold_standard_file_paths_testing]\n",
    "# Remove the rows that correspond to bad lines\n",
    "for i, gs in enumerate(gs_list_testing):\n",
    "    gs = gs.drop(bad_line_indices_list[i])\n",
    "\n",
    "# Preprocess the sentences\n",
    "for df in sentences_list_testing:\n",
    "    df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features (~2.4mins)\n",
    "features_list_testing = []\n",
    "for df in sentences_list_testing:\n",
    "    features_list_testing.append(get_features(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=14, max_features=&#x27;log2&#x27;, n_estimators=600,\n",
       "                      random_state=13)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=14, max_features=&#x27;log2&#x27;, n_estimators=600,\n",
       "                      random_state=13)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=14, max_features='log2', n_estimators=600,\n",
       "                      random_state=13)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = RandomForestRegressor(max_depth=14, max_features='log2', min_samples_leaf=1, min_samples_split=2, n_estimators=600,random_state=13)\n",
    "gs_training = np.ravel(gs_training)\n",
    "grid_search.fit(features_training, gs_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pearson correlation on SMTeuroparl: 0.579352793199067\n",
      "Testing pearson correlation on MSRvid: 0.8374729339421313\n",
      "Testing pearson correlation on MSRpar: 0.6147672752476367\n",
      "Testing pearson correlation on surprise.OnWN: 0.7240331029014654\n",
      "Testing pearson correlation on surprise.SMTnews: 0.5575948329412181\n",
      "Testing pearson correlation on all datasets: 0.738219448996194\n"
     ]
    }
   ],
   "source": [
    "# Dataset names\n",
    "dataset_names = [path.replace('../Data/Test/test-gold/STS.input.','').replace('.txt','') for path in sentences_file_paths_testing]\n",
    "\n",
    "features_list_testing\n",
    "\n",
    "pearson_correlations = []\n",
    "# Testing the model on each dataset\n",
    "for i, features in enumerate(features_list_testing):\n",
    "    predictions = grid_search.predict(features)\n",
    "    pearson_correlation = pearsonr(np.ravel(gs_list_testing[i]), predictions)[0]\n",
    "    print(f\"Testing pearson correlation on {dataset_names[i]}: {pearson_correlation}\")\n",
    "    pearson_correlations.append(pearson_correlation)\n",
    "\n",
    "# Testing the model on all datasets\n",
    "predictions = grid_search.predict(pd.concat(features_list_testing, ignore_index=True))\n",
    "pearson_correlation = pearsonr(np.ravel(pd.concat(gs_list_testing, ignore_index=True)), predictions)[0]\n",
    "print(f\"Testing pearson correlation on all datasets: {pearson_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features.txt\", \"w\") as file:\n",
    "    file = features_training.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ihlt)",
   "language": "python",
   "name": "ihlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
