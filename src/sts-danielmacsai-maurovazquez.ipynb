{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk import ngrams\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('sentiwordnet')\n",
    "ic_brown = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_of_elements(tokenized_sentence1, tokenized_sentence2) -> float:\n",
    "    \"\"\"Given two lists, return the ratio of the number of elements between the two lists\"\"\"\n",
    "    l1 = len(tokenized_sentence1)\n",
    "    l2 = len(tokenized_sentence2)\n",
    "    return min(l1,l2)/max(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levenstein_ratio(sentence1: str, sentence2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using the Levenshtein ratio.\n",
    "\n",
    "    The similarity is calculated as the ratio of the Levenshtein distance between the two sentences and the sum of the lengths of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Levenshtein ratio between the two sentences.\n",
    "    \"\"\"\n",
    "    return Levenshtein.ratio(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens\n",
    "def get_jaccard_similarity(sentence1,sentence2) -> float:\n",
    "    \"\"\"Returns Jaccard similarity between sentences, they could be tokenized or lemmatized\"\"\"\n",
    "    return 1-jaccard_distance(set(sentence1), set(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_methods = {\n",
    "    \"wu-palmer\": lambda s1, s2: s1.wup_similarity(s2),\n",
    "    \"path\": lambda s1, s2: s1.path_similarity(s2),\n",
    "    \"leacock\": lambda s1, s2: s1.lch_similarity(s2),\n",
    "    \"lin\": lambda s1, s2: s1.lin_similarity(s2, ic_brown)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(category):\n",
    "    \"\"\"\n",
    "    Convert a POS tag from the Spacy tagset to the WordNet tagset.\n",
    "    \"\"\"\n",
    "    if category.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif category.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif category.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif (category.startswith('R')) and (category != 'RP'):\n",
    "        # I looked into the RP tag is for particles\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return None  # WordNet doesn't handle other POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def get_best_synset_pair(word1, word2, pos, similarity_type) -> float:\n",
    "    \"\"\"\n",
    "    Get the best synset pair for two words.\n",
    "\n",
    "    This function gets the best synset pair for two words, considering every possible pair of synsets from the two words. The best pair is the one with the highest similarity score.\n",
    "\n",
    "    Parameters:\n",
    "    - word1: The first word, represented as a Spacy token.\n",
    "    - word2: The second word, represented as a Spacy token.\n",
    "    - pos: A string indicating the part of speech of the words.\n",
    "    - similarity_type: A string indicating the name of the similarity measure.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the similarity score of the best synset pair.\n",
    "    \"\"\"\n",
    "    # Create a cache key (I do this because it is simetric and to save time)\n",
    "    cache_key = tuple(sorted([word1.text, word2.text]) + [similarity_type]) \n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "\n",
    "    synsets_word1 = wordnet.synsets(word1.text, pos=pos)\n",
    "    synsets_word2 = wordnet.synsets(word2.text, pos=pos)\n",
    "\n",
    "    max_sim = 0\n",
    "    for synset1 in synsets_word1:\n",
    "        for synset2 in synsets_word2:\n",
    "            # We skip the satellite adjectives (gives problems for Lin and Leacock)\n",
    "            if synset1.pos() == 's' or synset2.pos() == 's':\n",
    "                continue\n",
    "            sim = similarity_methods[similarity_type](synset1, synset2)\n",
    "            if sim and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    \n",
    "    cache[cache_key] = max_sim\n",
    "    # If there is no similarity, we return 0\n",
    "    return max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_similarities(sentence1, sentence2, similarity_type) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using a specified similarity measure.\n",
    "\n",
    "    For each sentence, the function gets the best similarity value for each word considering every posible pair, using words from the other sentence. Then the mean of this similarities is computed over the sentence (normalizing by the number of tokens with a valid wordnet postag, not the total number of words) and the output is the mean similarity of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens.\n",
    "    - sentence2: The second sentence, represented as a list of tokens.\n",
    "    - similarity_type: A string indicating the type of similarity measure to use. \n",
    "        Options include \"wu-palmer\", \"path\", \"leacock\", and \"lin\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the average similarity score between the two sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do it for sentence1\n",
    "    similarity1 = 0\n",
    "    den = 0 # We will normalize by the number of words that have a valid POS for WordNet\n",
    "    for token1 in sentence1:\n",
    "        similarities = np.array([])\n",
    "        pos1 = get_wordnet_pos(token1.tag_)\n",
    "        if not pos1:\n",
    "            continue\n",
    "        den +=1\n",
    "        for token2 in sentence2:\n",
    "            pos2 = get_wordnet_pos(token2.tag_)\n",
    "            if (not pos2) or (pos1 != pos2):\n",
    "                continue\n",
    "            similarities = np.append(similarities,get_best_synset_pair(token1, token2, pos1, similarity_type))\n",
    "        if similarities.size > 0:\n",
    "            similarity1 += np.max(similarities)\n",
    "    # We average the similarity (even if they don't get a similarity)\n",
    "    similarity1 = similarity1 / den\n",
    "\n",
    "    # Do it for sentence2\n",
    "    similarity2 = 0\n",
    "    den = 0 # We will normalize by the number of words that have a valid POS for WordNet\n",
    "    for token2 in sentence2:\n",
    "        similarities = np.array([])\n",
    "        pos2 = get_wordnet_pos(token2.tag_)\n",
    "        if not pos2:\n",
    "            continue\n",
    "        den +=1\n",
    "        for token1 in sentence1:\n",
    "            pos1 = get_wordnet_pos(token1.tag_)\n",
    "            if (not pos1) or (pos1 != pos2):\n",
    "                continue\n",
    "            similarities = np.append(similarities, get_best_synset_pair(token1, token2, pos2, similarity_type))\n",
    "        if similarities.size > 0:\n",
    "            similarity2 += np.max(similarities)\n",
    "    # We average the similarity (even if they don't get a similarity)\n",
    "    similarity2 = similarity2 / den\n",
    "    \n",
    "    return np.mean(np.array([similarity1, similarity2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_jaccard_similarity(sentence1,sentence2) -> float:\n",
    "    \"\"\"\n",
    "    Returns the Lesk similarity between sentences\n",
    "    \n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens.\n",
    "    - sentence2: The second sentence, represented as a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Jaccard similarity between the best synsets of the two sentences, according to the Lesk algorithm.\n",
    "    \"\"\"\n",
    "    synsets1 = [lesk(sentence1, token.text, get_wordnet_pos(token.tag_)) for token in sentence1]\n",
    "    synsets2 = [lesk(sentence2, token.text, get_wordnet_pos(token.tag_)) for token in sentence2]\n",
    "    if len(synsets1) == 0 or len(synsets2) == 0:\n",
    "        return 0\n",
    "    return 1 - jaccard_distance(set(synsets1), set(synsets2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiwordnet_difference(sentence1,sentence2,method='pos'):\n",
    "    \"\"\"\n",
    "    This function calculates the absolute difference between the average sentiment scores of the lesk synsets of the two sentences, normalized by the maximum value of the sentiment score.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens.\n",
    "    - sentence2: The second sentence, represented as a list of tokens.\n",
    "    - method: A string indicating the type of sentiment score to use. \n",
    "        Options include \"pos\", \"neg\", and \"obj\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the difference in the mean sentiment scores of the two sentences, normalized by the maximum value of the sentiment score.\n",
    "    \"\"\"\n",
    "    synsets1 = [lesk(sentence1, token.text, get_wordnet_pos(token.tag_)) for token in sentence1]\n",
    "    synsets2 = [lesk(sentence2, token.text, get_wordnet_pos(token.tag_)) for token in sentence2]\n",
    "    \n",
    "    sentisynsets1 = [swn.senti_synset(synset.name()) for synset in synsets1 if synset is not None]\n",
    "    sentisynsets2 = [swn.senti_synset(synset.name()) for synset in synsets2 if synset is not None]\n",
    "\n",
    "    l1 = len(sentisynsets1)\n",
    "    l2 = len(sentisynsets2)\n",
    "\n",
    "    if len(l1) == 0 or len(l2) == 0:\n",
    "        return 0\n",
    "    \n",
    "    if method == 'pos':\n",
    "        sum1 = sum([sentiSynset.pos_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.pos_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    elif method == 'neg':\n",
    "        sum1 = sum([sentiSynset.neg_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.neg_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    elif method == 'obj':\n",
    "        sum1 = sum([sentiSynset.obj_score() for sentiSynset in sentisynsets1])\n",
    "        sum2 = sum([sentiSynset.obj_score() for sentiSynset in sentisynsets2])\n",
    "        if sum1 == 0 and sum2 == 0:\n",
    "            return abs(sum1 - sum2)\n",
    "        return abs(sum1 - sum2)/max(l1, l2)\n",
    "    else:\n",
    "        raise ValueError(\"Error: this method is not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word N-Grams Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_jaccard_similarity(sentence1, sentence2, n):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using n-grams. The similarity is calculated as the jaccard similarity between the set of n-grams of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens.\n",
    "    - sentence2: The second sentence, represented as a list of tokens.\n",
    "    - n: The size of the n-grams to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Jaccard similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    ngrams1 = set([str(x) for x in list(ngrams(sentence1,n))])\n",
    "    ngrams2 = set([str(x) for x in list(ngrams(sentence2,n))])\n",
    "    if len(ngrams1) == 0 and len(ngrams2) == 0:\n",
    "        return 1\n",
    "    return 1-jaccard_distance(ngrams1,ngrams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_vector_similarity(sentence1, sentence2, n):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using n-grams.\n",
    "\n",
    "    For each sentence, we count the number of times each n-gram appears in the sentence. Then we sum the minimum of the counts of each n-gram match, multiply it by 2 and divide it by the sum of the counts of all n-grams in the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a list of tokens/lemmas\n",
    "    - sentence2: The second sentence, represented as a list of tokens/lemmas\n",
    "    - n: The size of the n-grams to use.\n",
    "    \n",
    "    Returns:\n",
    "    - A float representing the similarity of the two sentences using n-grams.\n",
    "    \"\"\"\n",
    "    if len(sentence1) < n or len(sentence2) < n:\n",
    "        return 0\n",
    "    ngrams1 = [str(x) for x in list(ngrams(sentence1,n))]\n",
    "    ngrams2 = [str(x) for x in list(ngrams(sentence2,n))]\n",
    "    all_ngrams = set(ngrams1 + ngrams2)\n",
    "\n",
    "    c1 = Counter(ngrams1)\n",
    "    c2 = Counter(ngrams2)\n",
    "\n",
    "    coincidences = 0\n",
    "    for ngram in all_ngrams:\n",
    "        if ngram not in ngrams1 or ngram not in ngrams2:\n",
    "            continue\n",
    "        coincidences += min(c1[ngram],c2[ngram])\n",
    "    return 2*coincidences/(sum(c1.values())+sum(c2.values()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character N-Gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_ngram_jaccard_similarity(sentence1, sentence2, n):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two sentences using character n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using character n-grams. The similarity is calculated as the jaccard similarity between the set of n-grams of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "    - n: The size of the n-grams to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the Jaccard similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    ngrams1 = set([str(x) for x in list(ngrams(sentence1,n))])\n",
    "    ngrams2 = set([str(x) for x in list(ngrams(sentence2,n))])\n",
    "\n",
    "    return 1-jaccard_distance(ngrams1,ngrams2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_ngram_vector_cosine_similarity(sentence1: str, sentence2: str, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two sentences using character n-grams.\n",
    "\n",
    "    This function calculates the similarity between two sentences using character n-grams. \n",
    "    The similarity is calculated as the cosine similarity between the vectors of the n-grams of the two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1: The first sentence, represented as a string\n",
    "    - sentence2: The second sentence, represented as a string.\n",
    "    - nrange: A tuple representing the range of n-grams to use.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the cosine similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    n_range = (n, n)\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=n_range)\n",
    "    ngrams = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity(ngrams[0], ngrams[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get all of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features (df):\n",
    "\n",
    "    features = pd.DataFrame()\n",
    "    for rowindex, row in df.iterrows():\n",
    "        features.at[rowindex, 'token_ratio'] = ratio_of_number_of_tokens(row['0_nlp'], row['1_nlp'])\n",
    "        features.at[rowindex, 'char_ratio'] = ratio_of_number_of_characters(row[\"0_lower\"], row[\"1_lower\"])\n",
    "        features.at[rowindex, \"levenstein_ratio\"] = get_levenstein_ratio(row[\"0_lower\"], row[\"1_lower\"])\n",
    "\n",
    "        features.at[rowindex, 'jaccard_similarity_tokens'] = get_jaccard_similarity([token.text for token in row[\"0_nlp\"]], [token.text for token in row[\"1_nlp\"]])\n",
    "        features.at[rowindex, 'jaccard_similarity_tokens_no_stops'] = get_jaccard_similarity([token.text for token in row[\"0_nlp_no_stop\"]], [token.text for token in row[\"1_nlp_no_stop\"]])\n",
    "        features.at[rowindex, 'jaccard_similarity_lemmas'] = get_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"])\n",
    "        features.at[rowindex, 'jaccard_similarity_lemmas_no_stops'] = get_jaccard_similarity(row[\"0_lemma_no_stop\"], row[\"1_lemma_no_stop\"])\n",
    "\n",
    "        features.at[rowindex, 'wu-palmer_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"wu-palmer\")\n",
    "        #df.at[rowindex, 'lin_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"lin\")\n",
    "        features.at[rowindex, 'path_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"path\")\n",
    "        features.at[rowindex, 'leacock_similarity'] = get_sentence_similarities(row[\"0_nlp\"], row[\"1_nlp\"], \"leacock\")\n",
    "        features.at[rowindex, 'lesk_jaccard_similarity'] = lesk_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"])\n",
    "\n",
    "        features.at[rowindex, 'unigram_vector_cosine_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 1)\n",
    "        features.at[rowindex, 'bigram_vector_cosine_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'trigram_vector_cosine_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'quadgram_vector_cosine_similarity'] = get_ngram_vector_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        features.at[rowindex, 'token_bigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 2)\n",
    "        features.at[rowindex, 'token_trigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 3)\n",
    "        features.at[rowindex, 'token_quadgram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_nlp\"], row[\"1_nlp\"], 4)\n",
    "\n",
    "        features.at[rowindex, 'lemma_bigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 2)\n",
    "        features.at[rowindex, 'lemma_trigram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 3)\n",
    "        features.at[rowindex, 'lemma_quadgram_jaccard_similarity'] = get_ngram_jaccard_similarity(row[\"0_lemma\"], row[\"1_lemma\"], 4)\n",
    "\n",
    "        features.at[rowindex, 'char_bigram_jaccard_similarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'char_trigram_jaccard_similarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'char_quadgram_jaccard_simmilarity'] = get_character_ngram_jaccard_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        features.at[rowindex, 'char_bigram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 2)\n",
    "        features.at[rowindex, 'char_trigram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 3)\n",
    "        features.at[rowindex, 'char_quadgram_vector_cosine_similarity'] = get_char_ngram_vector_cosine_similarity(row[\"0_lower\"], row[\"1_lower\"], 4)\n",
    "\n",
    "        features.at[rowindex, 'sentiwordnet_pos_difference'] = sentiwordnet_correlation(row[\"0_nlp\"], row[\"1_nlp\"], 'pos')\n",
    "        features.at[rowindex, 'sentiwordnet_neg_difference'] = sentiwordnet_correlation(row[\"0_nlp\"], row[\"1_nlp\"], 'neg')\n",
    "        # features.at[rowindex, 'sentiwordnet_obj_difference'] = sentiwordnet_correlation(row[\"0_nlp\"], row[\"1_nlp\"], 'obj') # It is directly dependent on the other two\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences):\n",
    "    \"\"\"Processes the sentences to get the features\"\"\"\n",
    "    # Lowercase\n",
    "    sentences[\"0_lower\"] = sentences[0].apply(lambda x: ''.join([char.lower() if not char.isdigit() else char for char in x]))\n",
    "    sentences[\"1_lower\"] = sentences[1].apply(lambda x: ''.join([char.lower() if not char.isdigit() else char for char in x]))\n",
    "\n",
    "    # Tokens\n",
    "    sentences[\"0_nlp\"] = sentences[\"0_lower\"].apply(lambda x: nlp(x))\n",
    "    sentences[\"1_nlp\"] = sentences[\"1_lower\"].apply(lambda x: nlp(x))\n",
    "    # Remove punctuation\n",
    "    sentences[\"0_nlp\"] = sentences[\"0_nlp\"].apply(lambda x: [token for token in x if not token.is_punct])\n",
    "    sentences[\"1_nlp\"] = sentences[\"1_nlp\"].apply(lambda x: [token for token in x if not token.is_punct])\n",
    "    \n",
    "    # Filter out stop words\n",
    "    sentences[\"0_nlp_no_stop\"] = sentences[\"0_nlp\"].apply(lambda x: [token for token in x if not token.is_stop])\n",
    "    sentences[\"1_nlp_no_stop\"] = sentences[\"1_nlp\"].apply(lambda x: [token for token in x if not token.is_stop])\n",
    "\n",
    "    # Lemmas\n",
    "    sentences['0_lemma'] = sentences['0_nlp'].apply(lambda x: [token.lemma for token in x])\n",
    "    sentences['1_lemma'] = sentences['1_nlp'].apply(lambda x: [token.lemma for token in x])\n",
    "    # Filter out stop words\n",
    "    sentences['0_lemma_no_stop'] = sentences['0_nlp_no_stop'].apply(lambda x: [token.lemma for token in x])\n",
    "    sentences['1_lemma_no_stop'] = sentences['1_nlp_no_stop'].apply(lambda x: [token.lemma for token in x])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and clean data, tracking bad line indices\n",
    "def read_and_clean_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    bad_line_indices = []\n",
    "    for i, line in enumerate(lines):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields) == 2:\n",
    "            cleaned_lines.append(fields)\n",
    "        else:\n",
    "            print(f\"Skipping bad line: {line.strip()}\")\n",
    "            bad_line_indices.append(i)\n",
    "\n",
    "    return pd.DataFrame(cleaned_lines, columns=[0, 1]), bad_line_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "sentences_file_paths = [\n",
    "    \"../Data/Train/train/STS.input.SMTeuroparl.txt\",\n",
    "    \"../Data/Train/train/STS.input.MSRvid.txt\",\n",
    "    \"../Data/Train/train/STS.input.MSRpar.txt\",\n",
    "    ]\n",
    "gold_standard_file_paths = [\n",
    "    \"../Data/Train/train/STS.gs.SMTeuroparl.txt\",\n",
    "    \"../Data/Train/train/STS.gs.MSRvid.txt\",\n",
    "    \"../Data/Train/train/STS.gs.MSRpar.txt\",\n",
    "    ]   \n",
    "\n",
    "# Read and clean the data files into DataFrames, tracking bad line indices\n",
    "sentences_list = []\n",
    "bad_line_indices_list = []\n",
    "for path in sentences_file_paths:\n",
    "    df, bad_line_indices = read_and_clean_data(path)\n",
    "    sentences_list.append(df)\n",
    "    bad_line_indices_list.append(bad_line_indices)\n",
    "\n",
    "sentences_training: pd.DataFrame = pd.concat(sentences_list, ignore_index=True)\n",
    "\n",
    "gs_list = [pd.read_csv(path, header=None) for path in gold_standard_file_paths]\n",
    "# Remove the rows that correspond to bad lines\n",
    "for i, gs in enumerate(gs_list):\n",
    "    gs = gs.drop(bad_line_indices_list[i])\n",
    "gs_training: pd.DataFrame = pd.concat(gs_list, ignore_index=True)\n",
    "\n",
    "# Preprocess the sentences\n",
    "sentences_training = preprocessing(sentences_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features (~4mins)\n",
    "features_training = get_features(sentences_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = RandomForestRegressor(max_depth=14, max_features='log2', min_samples_leaf=1, min_samples_split=2, n_estimators=600,random_state=13)\n",
    "gs_training = np.ravel(gs_training)\n",
    "grid_search.fit(features_training, gs_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pearson correlation on SMTeuroparl: 0.5734560411396111\n",
      "Testing pearson correlation on MSRvid: 0.832564485571077\n",
      "Testing pearson correlation on MSRpar: 0.6124410320005863\n",
      "Testing pearson correlation on surprise.OnWN: 0.724718580699725\n",
      "Testing pearson correlation on surprise.SMTnews: 0.5574499423843311\n",
      "Testing pearson correlation on all datasets: 0.7263711560250121\n"
     ]
    }
   ],
   "source": [
    "# Dataset names\n",
    "dataset_names = [path.replace('../Data/Test/test-gold/STS.input.','').replace('.txt','') for path in sentences_file_paths_testing]\n",
    "\n",
    "features_list_testing\n",
    "\n",
    "pearson_correlations = []\n",
    "# Testing the model on each dataset\n",
    "for i, features in enumerate(features_list_testing):\n",
    "    predictions = grid_search.predict(features)\n",
    "    pearson_correlation = pearsonr(np.ravel(gs_list_testing[i]), predictions)[0]\n",
    "    print(f\"Testing pearson correlation on {dataset_names[i]}: {pearson_correlation}\")\n",
    "    pearson_correlations.append(pearson_correlation)\n",
    "\n",
    "# Testing the model on all datasets\n",
    "predictions = grid_search.predict(pd.concat(features_list_testing, ignore_index=True))\n",
    "pearson_correlation = pearsonr(np.ravel(pd.concat(gs_list_testing, ignore_index=True)), predictions)[0]\n",
    "print(f\"Testing pearson correlation on all datasets: {pearson_correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment PCA + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio by PCA components: [0.51138427 0.15756158 0.12970455 0.06382349 0.04985604 0.02740873\n",
      " 0.01613636 0.00887821 0.00783592 0.00496913 0.00422658 0.00416992\n",
      " 0.00279223 0.00243762]\n",
      "Number of components selected: 14\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA on the training features\n",
    "pca = PCA(n_components=0.99)  # Retain 95% of variance\n",
    "features_training_pca = pca.fit_transform(features_training)\n",
    "\n",
    "# Train a Random Forest Regressor on the PCA-transformed data\n",
    "random_forest = RandomForestRegressor(n_estimators=600, max_depth=14, max_features='log2', min_samples_leaf=1, min_samples_split=2, random_state=13)\n",
    "random_forest.fit(features_training_pca, gs_training)\n",
    "# Print the explained variance ratio to understand how much variance is retained\n",
    "print(f\"Explained variance ratio by PCA components: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pearson correlation on SMTeuroparl: 0.5224957854709418\n",
      "Testing pearson correlation on MSRvid: 0.8260860292089873\n",
      "Testing pearson correlation on MSRpar: 0.5832890181963831\n",
      "Testing pearson correlation on surprise.OnWN: 0.6936375786320211\n",
      "Testing pearson correlation on surprise.SMTnews: 0.5827772453424451\n",
      "Testing pearson correlation on all datasets: 0.7331939172964225\n"
     ]
    }
   ],
   "source": [
    "# Dataset names\n",
    "dataset_names = [path.replace('../Data/Test/test-gold/STS.input.','').replace('.txt','') for path in sentences_file_paths_testing]\n",
    "\n",
    "pearson_correlations = []\n",
    "# Testing the model on each dataset\n",
    "for i, features in enumerate(features_list_testing):\n",
    "    features_pca = pca.transform(features)\n",
    "    predictions = random_forest.predict(features_pca)\n",
    "    pearson_correlation = pearsonr(np.ravel(gs_list_testing[i]), predictions)[0]\n",
    "    print(f\"Testing pearson correlation on {dataset_names[i]}: {pearson_correlation}\")\n",
    "    pearson_correlations.append(pearson_correlation)\n",
    "\n",
    "# Testing the model on all datasets\n",
    "predictions = random_forest.predict(pca.transform(pd.concat(features_list_testing, ignore_index=True)))\n",
    "pearson_correlation = pearsonr(np.ravel(pd.concat(gs_list_testing,ignore_index=True)), predictions)[0]\n",
    "print(f\"Testing pearson correlation on all datasets: {pearson_correlation}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
