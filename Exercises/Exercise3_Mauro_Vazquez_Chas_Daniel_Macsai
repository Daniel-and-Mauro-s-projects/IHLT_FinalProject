{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["For this task we tried the NLTK and SpaCy tokenizers as well.\n"],"metadata":{"id":"1T25s8PlBGgv"}},{"cell_type":"code","metadata":{"id":"rV4ayENcxNeV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e045cba2-24f0-460b-f754-2ce865b179ea","executionInfo":{"status":"ok","timestamp":1727904685496,"user_tz":-120,"elapsed":4055,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"Mi9UgnrHx1NN","executionInfo":{"status":"ok","timestamp":1727904685496,"user_tz":-120,"elapsed":143,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"source":["import pandas as pd\n","from nltk.metrics import jaccard_distance"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"fx90zvqXx5eA","executionInfo":{"status":"ok","timestamp":1727904685496,"user_tz":-120,"elapsed":140,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"source":["df = pd.read_csv('/content/drive/MyDrive/IHLT/Exercises/STS.input.SMTeuroparl.txt',sep='\\t',header=None)"],"execution_count":35,"outputs":[]},{"cell_type":"code","source":["#We convert everything to lowercase\n","df[0]=df.apply(lambda x: x[0].lower(), axis = 1)\n","df[1]=df.apply(lambda x: x[1].lower(), axis = 1)"],"metadata":{"id":"XTTCPwlmI-uO","executionInfo":{"status":"ok","timestamp":1727904685497,"user_tz":-120,"elapsed":137,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["In the exercise for Lab 2 we already checked that by turning every word to lowercase the pearson coefficient improved in every approach."],"metadata":{"id":"y4SRC-dKzv_3"}},{"cell_type":"code","metadata":{"id":"4xWDSCAD0mZ_","executionInfo":{"status":"ok","timestamp":1727904685497,"user_tz":-120,"elapsed":134,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"source":["df['gs'] = pd.read_csv('/content/drive/MyDrive/IHLT/Exercises/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["We started with tokenizing in both approaches, since it is not possible to lemmatize after splitting the sentences by the whitespaces for example."],"metadata":{"id":"6q2h-cMAGxY8"}},{"cell_type":"markdown","source":["# With NLTK"],"metadata":{"id":"N4HYXEOIhdwf"}},{"cell_type":"markdown","source":["\n","\n","*   when we make decisions, make reasons for them (with results if possible)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"bEUwinMzjj_X"}},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger', quiet = True)\n","nltk.download('wordnet', quiet = True)\n","nltk.download('omw-1.4', quiet = True)\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1pbZtxOhfua","executionInfo":{"status":"ok","timestamp":1727904685497,"user_tz":-120,"elapsed":132,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"7f630e3d-54fc-4b57-8a16-0b14265b64ed"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["wnl = nltk.stem.WordNetLemmatizer()"],"metadata":{"id":"Qa-KMkWIh95c","executionInfo":{"status":"ok","timestamp":1727904685498,"user_tz":-120,"elapsed":128,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["## Non_Lemmatized: With tokenized version of the words\n"],"metadata":{"id":"jtFz4VSujJkL"}},{"cell_type":"markdown","source":["Here we present the previous result we got in Lab 2 (with tokenizing only):"],"metadata":{"id":"xAXpphjfHXnH"}},{"cell_type":"code","source":["df['doc_0_nltk'] = df.apply(lambda x: [token for token in nltk.word_tokenize(x[0])], axis = 1)\n","df['doc_1_nltk'] = df.apply(lambda x: [token for token in nltk.word_tokenize(x[1])], axis = 1)"],"metadata":{"id":"mOZ4mrTYjjSP","executionInfo":{"status":"ok","timestamp":1727904685498,"user_tz":-120,"elapsed":128,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["df['jaccard_nltk'] = df.apply(lambda x: 1-jaccard_distance(set(x['doc_0_nltk']), set(x['doc_1_nltk'])), axis = 1)"],"metadata":{"id":"m4kuUK6ek7fM","executionInfo":{"status":"ok","timestamp":1727904685498,"user_tz":-120,"elapsed":125,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['jaccard_nltk'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGq2Ynq1lwCK","executionInfo":{"status":"ok","timestamp":1727904685499,"user_tz":-120,"elapsed":124,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"e793d3a7-20ed-4e76-d82d-15cd09f399d3"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.46249513975914963"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["## Lemmatized: With tokenized version of the words\n"],"metadata":{"id":"w-2EfnNh3-HE"}},{"cell_type":"markdown","source":["Let's try to lemmatize the words using NLTK:"],"metadata":{"id":"qQTbwtz3Hlsn"}},{"cell_type":"code","source":["df['jaccard_nltk_lemma'] = df.apply(lambda x: 1-jaccard_distance(set(map(wnl.lemmatize,x['doc_0_nltk'])), set(map(wnl.lemmatize,x['doc_1_nltk']))), axis = 1)"],"metadata":{"id":"Q-JAuvWP3-HN","executionInfo":{"status":"ok","timestamp":1727904685500,"user_tz":-120,"elapsed":120,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["pearsonr(df['gs'], df['jaccard_nltk_lemma'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727904685500,"user_tz":-120,"elapsed":118,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"afd068c1-3671-4821-bce7-a9193e09e3d1","id":"mvG2nXO_3-HQ"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4707125711289244"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["As we expected, the results got better after lemmatizing."],"metadata":{"id":"O3f6Cz9q8KXd"}},{"cell_type":"markdown","source":["# With SpaCy"],"metadata":{"id":"F7QEKJwyiUjh"}},{"cell_type":"markdown","source":["Now repeating the whole process with SpaCy:"],"metadata":{"id":"hjPAvdk4Htrd"}},{"cell_type":"markdown","source":["First, let's do the previous approach (Lab 2), with only tokenizing:"],"metadata":{"id":"S8Knvie5H1Rt"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"llZc-JwYiV-T","executionInfo":{"status":"ok","timestamp":1727904686050,"user_tz":-120,"elapsed":665,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## Non_Lemmatized: With tokenized version of the words\n"],"metadata":{"id":"e8ie-tOy8aUY"}},{"cell_type":"code","source":["df['doc_0_spacy'] = df.apply(lambda x: [token.text for token in nlp(x[0])], axis = 1)\n","df['doc_1_spacy'] = df.apply(lambda x: [token.text for token in nlp(x[1])], axis = 1)"],"metadata":{"id":"pLHcprIn8aUb","executionInfo":{"status":"ok","timestamp":1727904695275,"user_tz":-120,"elapsed":9252,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["df['jaccard_spacy'] = df.apply(lambda x: 1-jaccard_distance(set(x['doc_0_spacy']), set(x['doc_1_spacy'])), axis = 1)"],"metadata":{"id":"T7AS7xxz8aUd","executionInfo":{"status":"ok","timestamp":1727904695518,"user_tz":-120,"elapsed":280,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['jaccard_spacy'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727904695519,"user_tz":-120,"elapsed":279,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"817aa615-ceb3-4976-8e7a-9c9991eff622","id":"foNjDxHo8aUg"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.47389169840204165"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["## Lemmatized: With tokenized version of the words\n"],"metadata":{"id":"2vpVV_Nk8aUi"}},{"cell_type":"code","source":["df['doc_0_spacy_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[0])], axis = 1)\n","df['doc_1_spacy_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[1])], axis = 1)"],"metadata":{"id":"dIi9j70o9ZFq","executionInfo":{"status":"ok","timestamp":1727904702764,"user_tz":-120,"elapsed":7521,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["df['jaccard_lemma_spacy'] = df.apply(lambda x: 1-jaccard_distance(set(x['doc_0_spacy_lemma']), set(x['doc_1_spacy_lemma'])), axis = 1)"],"metadata":{"id":"OKbiXMl28aUj","executionInfo":{"status":"ok","timestamp":1727904703062,"user_tz":-120,"elapsed":347,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["pearsonr(df['gs'], df['jaccard_lemma_spacy'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727904703063,"user_tz":-120,"elapsed":346,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"50ebf89f-b472-4bd7-dce9-8e409ab3a8bc","id":"xLaCJswP8aUk"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.48190657955388216"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["Lemmatizing also improved the results for pearson correlation. As Spacy was clearly better we will continue just using Spacy for the following heuristics."],"metadata":{"id":"TNUNDvN19smi"}},{"cell_type":"markdown","source":["## Non_Lemmatize: Filtering stopwords"],"metadata":{"id":"YxlP3bG8oHPJ"}},{"cell_type":"markdown","source":["The first heuristic we try is to filter out stopwords."],"metadata":{"id":"XtsPfNytIMwG"}},{"cell_type":"code","source":["df['filtered_0'] = df.apply(lambda x: [token.text for token in nlp(x[0]) if not token.is_stop], axis = 1)\n","df['filtered_1'] = df.apply(lambda x: [token.text for token in nlp(x[1]) if not token.is_stop], axis = 1)"],"metadata":{"id":"FMy0C_TnoKjF","executionInfo":{"status":"ok","timestamp":1727904713369,"user_tz":-120,"elapsed":10644,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["df['filtered_jaccard'] = df.apply(lambda x: 1-jaccard_distance(set(x['filtered_0']), set(x['filtered_1'])), axis = 1)"],"metadata":{"id":"-kOYOQwZoX0f","executionInfo":{"status":"ok","timestamp":1727904713388,"user_tz":-120,"elapsed":83,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['filtered_jaccard'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsoKM1ofojOe","executionInfo":{"status":"ok","timestamp":1727904713389,"user_tz":-120,"elapsed":82,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"28f6be12-1926-48f3-ceb4-e69dbd8f861d"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.47682625439777815"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["## Lemmatized: Filtering stopwords"],"metadata":{"id":"U5oQbVzy-op9"}},{"cell_type":"code","source":["df['filtered_0_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[0]) if not token.is_stop], axis = 1)\n","df['filtered_1_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[1]) if not token.is_stop], axis = 1)"],"metadata":{"id":"tTtwoTf2-oqH","executionInfo":{"status":"ok","timestamp":1727904720377,"user_tz":-120,"elapsed":7066,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["df['filtered_jaccard_lemma'] = df.apply(lambda x: 1-jaccard_distance(set(x['filtered_0_lemma']), set(x['filtered_1_lemma'])), axis = 1)"],"metadata":{"id":"coPKsx1i-oqJ","executionInfo":{"status":"ok","timestamp":1727904720378,"user_tz":-120,"elapsed":106,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['filtered_jaccard_lemma'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727904720380,"user_tz":-120,"elapsed":100,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"f0bc3988-7b4f-4938-bcd2-363e3beacfa1","id":"Lhw-0Jt--oqL"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49671900365312177"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["We see great improvement after removing stopwords both in the non-lemmatized an in the lemmatized versions and (again) lemmatizing is better."],"metadata":{"id":"X6KdnL3X--t_"}},{"cell_type":"markdown","source":["## Non-Lemmatizing: Removing punctuation marks as well"],"metadata":{"id":"xSMh1GzsuIdM"}},{"cell_type":"markdown","source":["After analysing a few examples, we saw that it might help if we removed the punctuation marks as well."],"metadata":{"id":"wxVncYXauN7E"}},{"cell_type":"code","source":["df['filtered_punct_0'] = df.apply(lambda x: [token.text for token in nlp(x[0]) if not token.is_stop or not token.is_punct], axis = 1)\n","df['filtered_punct_1'] = df.apply(lambda x: [token.text for token in nlp(x[1]) if not token.is_stop or not token.is_punct], axis = 1)"],"metadata":{"id":"Y707qDWcuYzy","executionInfo":{"status":"ok","timestamp":1727904730386,"user_tz":-120,"elapsed":10099,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["df['filtered_punct_jaccard'] = df.apply(lambda x: 1-jaccard_distance(set(x['filtered_punct_0']), set(x['filtered_punct_1'])), axis = 1)"],"metadata":{"id":"pDNImfTOumcT","executionInfo":{"status":"ok","timestamp":1727904730388,"user_tz":-120,"elapsed":120,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['filtered_punct_jaccard'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_AIOqrDutJ2","executionInfo":{"status":"ok","timestamp":1727904730389,"user_tz":-120,"elapsed":114,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"5c1a0a8f-46a0-488e-98ef-baac49ee3890"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.47389169840204165"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["## Lemmatizing: Removing punctuation marks as well"],"metadata":{"id":"TdLfuaTs_eHG"}},{"cell_type":"code","source":["df['filtered_punct_0_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[0]) if not token.is_stop or not token.is_punct], axis = 1)\n","df['filtered_punct_1_lemma'] = df.apply(lambda x: [token.lemma for token in nlp(x[1]) if not token.is_stop or not token.is_punct], axis = 1)"],"metadata":{"id":"yxhLSrLW_eHI","executionInfo":{"status":"ok","timestamp":1727904743427,"user_tz":-120,"elapsed":13144,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["df['filtered_punct_jaccard_lemma'] = df.apply(lambda x: 1-jaccard_distance(set(x['filtered_punct_0_lemma']), set(x['filtered_punct_1_lemma'])), axis = 1)"],"metadata":{"id":"76HnOFiE_eHL","executionInfo":{"status":"ok","timestamp":1727904743436,"user_tz":-120,"elapsed":182,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['filtered_punct_jaccard_lemma'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727904743438,"user_tz":-120,"elapsed":180,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"060b3e07-20f3-49ed-b74a-2d8aa5287a5c","id":"DbTpxFwF_eHM"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.48190657955388216"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["We notice the same behaviour as in the non-lemmatize case, the pearson coeficient decreases after removing punctuation in both cases. Apparently, this didn't help us get closer to the golden standard."],"metadata":{"id":"p4-EJ07q_xW2"}},{"cell_type":"markdown","source":["To conclude, we got the best results when we removed stopwords after using the SpaCy lemmatizer. This proves, that on average, lemmatizing brings us closer to the golden standars. There are counterexamples, however. See below."],"metadata":{"id":"hhJt885JFlh5"}},{"cell_type":"markdown","source":["An example where lemmatizing is worse:\n","\n","Consider the similarity between the sentences:\n","\n","\"I was being careful\" and \"I am careful\".\n","\n","If we lemmatize them, they both become {I, be, careful}, so according to the Jaccard similarity, they are exactly the same, but they are indeed not - we would expect the golden standard to be less than 1.\n"],"metadata":{"id":"LaoatjdzBtg2"}},{"cell_type":"code","source":["print(set([token.lemma_ for token in nlp(\"I was being careful\")]))\n","print(set([token.lemma_ for token in nlp(\"I am careful\")]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qmaGeiuBrh2","executionInfo":{"status":"ok","timestamp":1727904743439,"user_tz":-120,"elapsed":170,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"4e64e98d-06f2-4819-e801-2f65b58c0a41"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["{'I', 'careful', 'be'}\n","{'I', 'careful', 'be'}\n"]}]},{"cell_type":"markdown","source":["We can also have sentences where the Jaccard similarity decreases because of the lemmatization:\n","\n","\"I was being nice\" and \"I was being careful\".\n","\n","After lemmatizing, the Jaccard similarity decreases to 2/3 from 3/4. We are not sure if this is an improvement or not, because we would need the golden standard for these sentences.\n"],"metadata":{"id":"kJWdpyIADnw3"}},{"cell_type":"code","source":["print(set([token.lemma_ for token in nlp(\"I was being careful\")]))\n","print(set([token.lemma_ for token in nlp(\"I was being nice\")]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dpGptmWCtD-","executionInfo":{"status":"ok","timestamp":1727904743441,"user_tz":-120,"elapsed":159,"user":{"displayName":"Mauro Vázquez Chas","userId":"09295638050179013463"}},"outputId":"aaa5eeb9-6c37-4445-d12e-9e5f4128681c"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["{'I', 'careful', 'be'}\n","{'I', 'nice', 'be'}\n"]}]}]}