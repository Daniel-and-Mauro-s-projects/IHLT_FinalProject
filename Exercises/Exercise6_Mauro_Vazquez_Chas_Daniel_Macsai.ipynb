{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise 6"],"metadata":{"id":"YHoZZdYPnUzZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EGDfswelBZF","executionInfo":{"status":"ok","timestamp":1729720637707,"user_tz":-120,"elapsed":14521,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"8ead01ec-2bdc-4f58-a3d7-01deb5886faf"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","from nltk.metrics import jaccard_distance"]},{"cell_type":"code","source":["!pip install -U spacy\n","import spacy\n","from scipy.stats import pearsonr"],"metadata":{"id":"qqDkSZ2-tMXE","executionInfo":{"status":"ok","timestamp":1729720673695,"user_tz":-120,"elapsed":35992,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cef256d-f51b-40ac-b9cf-4b0d726c3c7f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Collecting spacy\n","  Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n","  Downloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n","Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n","  Downloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n","Collecting numpy>=1.19.0 (from spacy)\n","  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Downloading spacy-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading thinc-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blis-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: blis\n","    Found existing installation: blis 0.7.11\n","    Uninstalling blis-0.7.11:\n","      Successfully uninstalled blis-0.7.11\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.2.5\n","    Uninstalling thinc-8.2.5:\n","      Successfully uninstalled thinc-8.2.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.7.5\n","    Uninstalling spacy-3.7.5:\n","      Successfully uninstalled spacy-3.7.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n","en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.2 which is incompatible.\n","gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n","pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n","tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed blis-1.0.1 numpy-2.0.2 spacy-3.8.2 thinc-8.3.2\n"]}]},{"cell_type":"markdown","source":["## Importing the files"],"metadata":{"id":"yY7Ygvw-FbUS"}},{"cell_type":"code","source":["from google.colab import drive\n","import sys"],"metadata":{"id":"PbePJRY5l2WV","executionInfo":{"status":"ok","timestamp":1729720673696,"user_tz":-120,"elapsed":7,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvC48Bl0l2TF","executionInfo":{"status":"ok","timestamp":1729720696526,"user_tz":-120,"elapsed":22836,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"c7d79d23-d1f4-4287-d917-84f8d5b27e61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**We decided not to use textserver because it had a limit for the queries and we couldn't use it for all the sentences**"],"metadata":{"id":"avKymqUfE8O3"}},{"cell_type":"code","metadata":{"id":"fx90zvqXx5eA","executionInfo":{"status":"ok","timestamp":1729720706070,"user_tz":-120,"elapsed":1496,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Exercises/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n","#df = pd.read_csv('/content/drive/MyDrive/IHLT/Exercises/STS.input.SMTeuroparl.txt',sep='\\t',header=None)"],"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#We convert everything to lowercase\n","df[0]=df.apply(lambda x: x[0].lower(), axis = 1)\n","df[1]=df.apply(lambda x: x[1].lower(), axis = 1)"],"metadata":{"id":"XTTCPwlmI-uO","executionInfo":{"status":"ok","timestamp":1729720710210,"user_tz":-120,"elapsed":297,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xWDSCAD0mZ_","executionInfo":{"status":"ok","timestamp":1729720717109,"user_tz":-120,"elapsed":1220,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"source":["df['gs'] = pd.read_csv('/content/drive/MyDrive/Exercises/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n","#df['gs'] = pd.read_csv('/content/drive/MyDrive/IHLT/Exercises/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Original results from exercise 2"],"metadata":{"id":"w94bLZzqHgCV"}},{"cell_type":"markdown","source":["We now present our best results from Exercise 2."],"metadata":{"id":"uzr-RkkgspoL"}},{"cell_type":"code","source":["df['nltk_tokenized_0'] = df.apply(lambda x: [token for token in nltk.word_tokenize(x[0])], axis = 1)\n","df['nlkt_tokenized_1'] = df.apply(lambda x: [token for token in nltk.word_tokenize(x[1])], axis = 1)"],"metadata":{"id":"mOZ4mrTYjjSP","executionInfo":{"status":"ok","timestamp":1729720718138,"user_tz":-120,"elapsed":702,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df['jaccard_exercise_2'] = df.apply(lambda x: 1-jaccard_distance(set(x['nltk_tokenized_0']), set(x['nlkt_tokenized_1'])), axis = 1)"],"metadata":{"id":"m4kuUK6ek7fM","executionInfo":{"status":"ok","timestamp":1729720718139,"user_tz":-120,"elapsed":8,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import pearsonr\n","pearsonr(df['gs'], df['jaccard_exercise_2'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGq2Ynq1lwCK","executionInfo":{"status":"ok","timestamp":1729720718139,"user_tz":-120,"elapsed":7,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"ddc60257-377f-464d-ce2f-dbb7ac65475e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.46249513975914963"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Original results from exercise 3"],"metadata":{"id":"s655w8LQGubS"}},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"EkcIW9aBHBq3","executionInfo":{"status":"ok","timestamp":1729720720703,"user_tz":-120,"elapsed":1398,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"adf73b40-f571-41db-9b4d-ff662d781971"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n","  warnings.warn(warn_msg)\n"]}]},{"cell_type":"code","source":["df['lemmatized_filtered_0'] = df.apply(lambda x: [token.lemma for token in nlp(x[0]) if not token.is_stop], axis = 1)\n","df['lemmatized_filtered_1'] = df.apply(lambda x: [token.lemma for token in nlp(x[1]) if not token.is_stop], axis = 1)"],"metadata":{"id":"tTtwoTf2-oqH","executionInfo":{"status":"ok","timestamp":1729720728421,"user_tz":-120,"elapsed":7719,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["df['jaccard_exercise_3'] = df.apply(lambda x: 1-jaccard_distance(set(x['lemmatized_filtered_0']), set(x['lemmatized_filtered_1'])), axis = 1)"],"metadata":{"id":"coPKsx1i-oqJ","executionInfo":{"status":"ok","timestamp":1729720728421,"user_tz":-120,"elapsed":10,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["pearsonr(df['gs'], df['jaccard_exercise_3'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729720728422,"user_tz":-120,"elapsed":10,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"d4e98c1c-ef58-4d6f-e0df-c27adde8e223","id":"Lhw-0Jt--oqL"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49671900365312177"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## WSD Analysis"],"metadata":{"id":"ecoRM0cVF_k9"}},{"cell_type":"markdown","source":["We decided to tokenize the words from the sentences before calling the WSD algorithm."],"metadata":{"id":"82_ZkEUmPTMo"}},{"cell_type":"markdown","source":["### Directlly using all tokens"],"metadata":{"id":"PD1Jspi5WWkb"}},{"cell_type":"code","source":["def get_wordnet_pos(category):\n","  '''Give the POS-tag used by wordnet from SpaCy's POS-tags.'''\n","  if category.startswith('ADJ'):\n","    return 'a'  # Adjective\n","  elif category.startswith('V'):\n","    return 'v'  # Verb\n","  elif category.startswith('N'):\n","    return 'n'  # Noun\n","  elif category.startswith('ADV'):\n","    return 'r'  # Adverb\n","  else:\n","    return None  # WordNet doesn't handle other POS tags"],"metadata":{"id":"cjUVu0BDKeHa","executionInfo":{"status":"ok","timestamp":1729720738196,"user_tz":-120,"elapsed":342,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["df['synsets_0'] = df.apply(lambda x: [nltk.wsd.lesk([token.text for token in nlp(x[0])], token.text, get_wordnet_pos(token.pos_)) for token in nlp(x[0])], axis = 1)"],"metadata":{"id":"Y2L4bVfKl2P1","executionInfo":{"status":"ok","timestamp":1729720786320,"user_tz":-120,"elapsed":47832,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["df['synsets_1'] = df.apply(lambda x: [nltk.wsd.lesk([token.text for token in nlp(x[1])], token.text, get_wordnet_pos(token.pos_)) for token in nlp(x[1])], axis = 1)"],"metadata":{"id":"dQc6V_B7PFUH","executionInfo":{"status":"ok","timestamp":1729720838365,"user_tz":-120,"elapsed":52047,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["We decided to remove the None-s from the list of synsets for each pair of sentences, which turned out to perform better."],"metadata":{"id":"4cxByYTPuWyv"}},{"cell_type":"code","source":["df['jaccard_synsets_naive'] = df.apply(lambda x: 1-jaccard_distance(set(x['synsets_0'])-{None}, set(x['synsets_1'])-{None}), axis = 1)\n","df['jaccard_synsets_naive_with_none'] = df.apply(lambda x: 1-jaccard_distance(set(x['synsets_0']), set(x['synsets_1'])), axis = 1)"],"metadata":{"id":"S_MQNn6pRm9H","executionInfo":{"status":"ok","timestamp":1729720838366,"user_tz":-120,"elapsed":7,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(pearsonr(df['gs'], df['jaccard_synsets_naive'])[0])\n","print(pearsonr(df['gs'], df['jaccard_synsets_naive_with_none'])[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gfITMJhSHJ8","executionInfo":{"status":"ok","timestamp":1729720838366,"user_tz":-120,"elapsed":7,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"87c11fa2-5bee-41d5-ec9f-17074a40f06f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4415824008205342\n","0.42986490022193113\n"]}]},{"cell_type":"markdown","source":["As we expected, the results are not great at all, comparing naively the sets of the synsets is a super simple idea.\n","\n","Later we will try a more sophisticated method for comparing the list of synsets.\n","A deeper problem is that the synsets provided by Lesk's algorithm are often wrong. This is especially the case in words that don't have an nltk POS-tag, because they are often given a synset that doesn't match the word category."],"metadata":{"id":"EsJ6TJuBTLM5"}},{"cell_type":"markdown","source":["### Using only tokens with a pos tag in WorldNet"],"metadata":{"id":"jpv0ODAUWegT"}},{"cell_type":"markdown","source":["To solve the problem shown above, we remove those words completely which are not adjectives, verbs, nouns or adverbs (since these are the ones which can be used with WSD)."],"metadata":{"id":"2Xprn2VywkMD"}},{"cell_type":"code","source":["df['synsets_0_filtered'] = df.apply(lambda x: [nltk.wsd.lesk([token.text for token in nlp(x[0])], token.text, get_wordnet_pos(token.pos_)) for token in nlp(x[0]) if get_wordnet_pos(token.pos_)], axis = 1)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1729720868106,"user_tz":-120,"elapsed":29744,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"id":"mYZ-pj0-WtmD"},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["df['synsets_1_filtered'] = df.apply(lambda x: [nltk.wsd.lesk([token.text for token in nlp(x[1])], token.text, get_wordnet_pos(token.pos_)) for token in nlp(x[1]) if get_wordnet_pos(token.pos_)], axis = 1)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1729720889635,"user_tz":-120,"elapsed":21535,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"id":"a60hS4qlWtmH"},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# We create a function to filter the lists with no elements, since the jaccard distance returns an error in that case\n","def jaccard_similarity_filtered(x):\n","  try:\n","    return 1-jaccard_distance(set(x['synsets_0_filtered']), set(x['synsets_1_filtered']))\n","  except:\n","    return 0"],"metadata":{"id":"tiSxY0ZUkSyk","executionInfo":{"status":"ok","timestamp":1729720889636,"user_tz":-120,"elapsed":11,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["df['jaccard_synsets_filtered'] = df.apply(jaccard_similarity_filtered, axis = 1)\n","# if they are empty we save 0 as the simmilarity"],"metadata":{"executionInfo":{"status":"ok","timestamp":1729720889636,"user_tz":-120,"elapsed":10,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"id":"rSvqcSpEWtmH"},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["pearsonr(df['gs'], df['jaccard_synsets_filtered'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729720889636,"user_tz":-120,"elapsed":9,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"4ce6de14-c6fe-4a42-fb39-b410abfc708b","id":"W6chtJprWtmI"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.35843957260481935"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["Let's get a sense of how many elements we lose on average due to lack of POS-tags."],"metadata":{"id":"rG3nKJ7umIUo"}},{"cell_type":"code","source":["# Non-filtered lists\n","mean0=0\n","mean1=0\n","for i in range(len(df['synsets_0'])):\n","  mean0+=len(df.loc[i,'synsets_0'])\n","  mean1+=len(df.loc[i,'synsets_1'])\n","print(f\"The mean number of elements of the non-filtered lists is: {round(mean0/459,2)} and {round(mean1/459,2)}\")\n","\n","# Filtered lists\n","mean0=0\n","mean1=0\n","for i in range(len(df['synsets_0_filtered'])):\n","  mean0+=len(df.loc[i,'synsets_0_filtered'])\n","  mean1+=len(df.loc[i,'synsets_1_filtered'])\n","print(f\"The mean number of elements of the filtered lists is: {round(mean0/459,2)} and {round(mean1/459,2)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLk6jDR0mPMM","executionInfo":{"status":"ok","timestamp":1729721022567,"user_tz":-120,"elapsed":460,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"4a80bc75-6998-43ae-f42a-84ad5edc143b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["The mean number of elements of the non-filtered lists is: 12.19 and 12.67\n","The mean number of elements of the filtered lists is: 5.52 and 5.38\n"]}]},{"cell_type":"markdown","source":["* We get an unexpected outcome: the results got worse when we removed the synsets of words without a valid postag. We think that the explanation is that when you don't provide a postag, the WSD algorithm looks into every posible synset without taking into account the type of word. Unexpectedly, this blind choice makes our similarity score correlate more with the gold standard.\n","\n","* There's a huge loss of synsets caused by the removal of words without POS-tags (we lose more than half). This could also account to the results being worse after removal.\n","\n","* Up until this, when using the Jaccard similarity for two sets, we only checked if too synsets are exactly the same or not. To improve results we should make use of the fact that we can compare similarities between synsets as we did in the previous week. We will use Wu-Palmer similarity for this."],"metadata":{"id":"DMkiRQ_IXUB0"}},{"cell_type":"markdown","source":["### Keeping the tokens for words without POS-tags"],"metadata":{"id":"NATrrp3-KXUh"}},{"cell_type":"markdown","source":["In this section we test that we don't perform the WSD algorithm for words which are not verbs, adverbs, nouns or adjectives but we also don't throw them away: we keep the tokens for the set. This way, the set for each sentence will contain tokens and synsets as well."],"metadata":{"id":"mB2ShOXCZVpI"}},{"cell_type":"code","source":["# We create a function to keep the tokens without postag\n","def synsets_token_coscious(x,i):\n","  return [\n","    nltk.wsd.lesk([token.text for token in nlp(x[0])], token.text, get_wordnet_pos(token.pos_)) if get_wordnet_pos(token.pos_)\n","    else token.text\n","    for token in nlp(x[i])\n","  ]"],"metadata":{"executionInfo":{"status":"ok","timestamp":1729721031298,"user_tz":-120,"elapsed":338,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"id":"BSMsCGA-Kgye"},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["df['synsets_tokens_0'] = df.apply(synsets_token_coscious,i=0, axis = 1)"],"metadata":{"id":"D2Rxiy4BKbLh","executionInfo":{"status":"ok","timestamp":1729721056413,"user_tz":-120,"elapsed":24753,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["df['synsets_tokens_1'] = df.apply(synsets_token_coscious,i=1, axis = 1)"],"metadata":{"id":"PRnTazVUMNQW","executionInfo":{"status":"ok","timestamp":1729721078831,"user_tz":-120,"elapsed":22423,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["df['jaccard_synsets_tokens'] = df.apply(lambda x: 1-jaccard_distance(set(x['synsets_tokens_0']), set(x['synsets_tokens_1'])), axis = 1)\n","pearsonr(df['gs'], df['jaccard_synsets_tokens'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4yPh9TeNjAd","executionInfo":{"status":"ok","timestamp":1729721078832,"user_tz":-120,"elapsed":7,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"c5c6ba72-3a02-4671-cd66-6f33ecddd2bb"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4807679545867132"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["We see that after doing this the result improves, but still far from the one obtained by lemmatizing."],"metadata":{"id":"7Y6UWRLNOjPe"}},{"cell_type":"markdown","source":["### Wu-Palmer similarity computation"],"metadata":{"id":"QR__pSJRjMoI"}},{"cell_type":"markdown","source":["Our idea was to :\n","- iterate through the synsets from the first sentence,\n","- find the most similar synset from the second sentence for each synset in the first sencence,\n","- save this best similarity value for all of the synsets from the first sentence,\n","- and finally, average them across the first sentence.\n","\n","This would be our similarity value for the two sentences."],"metadata":{"id":"n1uXeRrF5C5h"}},{"cell_type":"code","source":["### Using Wu-Palmer similarity insted of jaccard similarity\n","for i, synset_list in enumerate(df['synsets_0']):\n","  wu_mean=0\n","\n","  for synset in set(synset_list)-{None}:\n","    maximum=0\n","\n","    for synset2 in set(df['synsets_1'][i])-{None}:\n","      try:\n","        wup=synset.wup_similarity(synset2)\n","        if wup>maximum:\n","          maximum=wup\n","      except AttributeError:\n","        print('The synsets were none')\n","    wu_mean+=maximum\n","  df.loc[i,'Wu-Palmer Similarity']=wu_mean/len(synset_list)"],"metadata":{"id":"z_8fh96wb3ot","executionInfo":{"status":"ok","timestamp":1729721084514,"user_tz":-120,"elapsed":5685,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["pearsonr(df['gs'], df['Wu-Palmer Similarity'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qwv0n6dWR-bQ","executionInfo":{"status":"ok","timestamp":1729721084514,"user_tz":-120,"elapsed":4,"user":{"displayName":"Dániel Mácsai","userId":"14906705828771596691"}},"outputId":"45313085-00af-41ac-823b-c6e365d86413"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.28109582443789133"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["This result is far worse than we expected, we had high hopes for utilizing the Wu-Palmer similarity. In the end, the best results we have so far are from Week 3, using lemmatization and filtering the stopwords."],"metadata":{"id":"-6zQKza_qTDx"}}]}